{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91344ef-72b8-407c-a56a-910b306e07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Matheus Gomes Correia\n",
    "# License: CC-BY-NC-SA 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9428f-27ae-465c-a2fd-8c9287f9c42c",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388542e-7882-4635-897f-ab5f014957b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import entropy as calculate_entropy\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import gc\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec666db-3a05-4b44-9992-264bcebb65b6",
   "metadata": {},
   "source": [
    "# Define Constants and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b7dfa-4233-4897-8fce-35c657219939",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.path.abspath('')\n",
    "PROJECT_ROOT = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Define all directories based on the Project Root\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\") # The folder where city subfolders are located\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\n",
    "FIGURES_DIR = os.path.join(RESULTS_DIR, \"figures\", \"clustering\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True) \n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "all_city_gdfs = []\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Data will be loaded from: {DATA_DIR}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "print(f\"Figures will be saved to: {FIGURES_DIR}\")\n",
    "print(\"-\" * 20)\n",
    "print(\"Searching for processed network files...\")\n",
    "\n",
    "# Find all the individual city network files generated by the previous script\n",
    "network_files = glob.glob(os.path.join(DATA_DIR, \"*\", \"*_processed_network.gpkg\"))\n",
    "\n",
    "if not network_files:\n",
    "    raise FileNotFoundError(f\"No processed network files (*_processed_network.gpkg) found in the subdirectories of '{DATA_DIR}'. Please run the first script to generate them.\")\n",
    "\n",
    "print(f\"Found {len(network_files)} city network files. Loading and combining...\")\n",
    "\n",
    "for filepath in network_files:\n",
    "    try:\n",
    "        # --- Extract city_id from the folder path ---\n",
    "        city_id = os.path.basename(os.path.dirname(filepath))\n",
    "\n",
    "        gdf_city = gpd.read_file(filepath)\n",
    "        if gdf_city.empty:\n",
    "            print(f\"  - Skipping empty file: {filepath}\")\n",
    "            continue\n",
    "\n",
    "        # --- Create the 'unique_edge_id' ---\n",
    "        # Combining the city_id with the row index from its own file.\n",
    "        # This creates a unique and traceable ID for every road segment.\n",
    "        gdf_city = gdf_city.reset_index(drop=True)\n",
    "        gdf_city['unique_edge_id'] = f\"{city_id}_\" + gdf_city.index.astype(str)\n",
    "\n",
    "        # Add city_id as a regular column for easy filtering later\n",
    "        gdf_city['city_id'] = city_id\n",
    "        \n",
    "        all_city_gdfs.append(gdf_city)\n",
    "        print(f\"  - Loaded {city_id} ({len(gdf_city)} edges)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  - FAILED to load or process {filepath}. Error: {e}\")\n",
    "\n",
    "# --- Combine all cities into a single GeoDataFrame ---\n",
    "if not all_city_gdfs:\n",
    "    raise ValueError(\"No city data could be loaded. Cannot proceed.\")\n",
    "\n",
    "df = pd.concat(all_city_gdfs, ignore_index=True)\n",
    "\n",
    "print(\"\\nFull dataset combined successfully.\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Memory usage (approx):\", df.memory_usage(deep=True).sum() / (1024**2), \"MB\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nData head (note the new 'unique_edge_id' and 'city_id' columns):\")\n",
    "print(df[['unique_edge_id', 'highway', 'length', 'city_id', 'geometry']].head())\n",
    "\n",
    "\n",
    "# --- FILTERING STEP ---\n",
    "# Uncomment this whole part to help debugging or only perform clustering with a single city\n",
    "\n",
    "#city_to_keep = 'fortaleza'\n",
    "#print(f\"\\nFiltering for rows where 'city_id' is '{city_to_keep}'...\")\n",
    "\n",
    "#if 'city_id' not in df.columns:\n",
    "#    raise ValueError(\"Column 'city_id' not found in the DataFrame!\")\n",
    "\n",
    "#df = df[df['city_id'] == city_to_keep].copy()\n",
    "\n",
    "#print(f\"Filtering complete.\")\n",
    "#print(\"Filtered dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c3142-efd7-4e7b-b49c-a360496309e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Top 50 Value Counts for Each Column (Raw Data) ---\")\n",
    "for column in df.columns:\n",
    "    print(f\"\\n--- Column: {column} ---\")\n",
    "    # Using dropna=False to see NaNs if they are frequent\n",
    "    value_counts_series = df[column].value_counts(dropna=False).head(50)\n",
    "    if value_counts_series.empty:\n",
    "        print(\"No values in this column (or all are unique and >50).\")\n",
    "    else:\n",
    "        # For better readability, especially if values are long strings\n",
    "        with pd.option_context('display.max_colwidth', None, 'display.width', 1000):\n",
    "            print(value_counts_series)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74cc104-0bfe-4675-a981-4228b2f281d3",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34e16a-2a76-4847-bda5-4ee77fadcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Cleaning 'lanes' column ---\")\n",
    "\n",
    "def clean_lanes_value(val):\n",
    "    if pd.isna(val):  # Handles np.nan, pd.NA, None\n",
    "        return np.nan\n",
    "\n",
    "    # Convert to string and strip whitespace, as 'lanes' was read with dtype=str\n",
    "    s_val = str(val).strip()\n",
    "\n",
    "    if not s_val:  # Handles empty strings\n",
    "        return np.nan\n",
    "\n",
    "    # Method 1: Direct conversion to integer (e.g., \"2\", \"3\")\n",
    "    try:\n",
    "        return int(s_val)\n",
    "    except ValueError:\n",
    "        pass  # Not a simple integer string, try next method\n",
    "\n",
    "    # Method 2: Conversion to float, then floor (e.g., \"1.5\" -> 1, \"2.0\" -> 2)\n",
    "    try:\n",
    "        float_val = float(s_val)\n",
    "        return int(np.floor(float_val))\n",
    "    except ValueError:\n",
    "        pass  # Not a float string, try next method\n",
    "\n",
    "    # Method 3: Parse list-like strings (e.g., \"['2', '3']\", \"['4','2','3']\")\n",
    "    if s_val.startswith('[') and s_val.endswith(']'):\n",
    "        try:\n",
    "            # ast.literal_eval safely parses string representations of Python literals\n",
    "            parsed_list = ast.literal_eval(s_val)\n",
    "\n",
    "            if not isinstance(parsed_list, list): # Ensure it was a list\n",
    "                return np.nan\n",
    "\n",
    "            numbers_in_list = []\n",
    "            for item in parsed_list:\n",
    "                try:\n",
    "                    # Convert item to float (handles int, float, or string numbers like '2', '1.5')\n",
    "                    # Then floor and convert to int\n",
    "                    num = float(item)\n",
    "                    numbers_in_list.append(int(np.floor(num)))\n",
    "                except (ValueError, TypeError):\n",
    "                    # If an item in the list is not a number (e.g., 'yes', None within list)\n",
    "                    # Mark the entire original list-string as problematic\n",
    "                    return np.nan\n",
    "            \n",
    "            if numbers_in_list: # If we successfully extracted numbers\n",
    "                return min(numbers_in_list)\n",
    "            else:\n",
    "                # List was empty or all items were non-numeric after parsing\n",
    "                return np.nan\n",
    "        except (ValueError, SyntaxError, TypeError):\n",
    "            # ast.literal_eval failed (e.g., malformed list string like \"[1, bad, 2]\")\n",
    "            # or an unexpected type was encountered during item processing\n",
    "            return np.nan\n",
    "            \n",
    "    # If none of the above patterns matched, it's a \"weird value\"\n",
    "    return np.nan\n",
    "\n",
    "# Apply the cleaning function to the 'lanes' column\n",
    "df['lanes_cleaned'] = df['lanes'].apply(clean_lanes_value)\n",
    "\n",
    "print(\"\\nValue counts for 'lanes_cleaned' (Top 50):\")\n",
    "print(df['lanes_cleaned'].value_counts(dropna=False).head(50))\n",
    "print(f\"\\nData type of 'lanes_cleaned': {df['lanes_cleaned'].dtype}\")\n",
    "\n",
    "# Sanity check: Count how many original NaNs are still NaNs\n",
    "original_nans = df['lanes'].isna().sum()\n",
    "cleaned_nans = df['lanes_cleaned'].isna().sum()\n",
    "print(f\"Original NaNs in 'lanes': {original_nans}\")\n",
    "print(f\"NaNs in 'lanes_cleaned' (includes original NaNs and newly created NaNs from unparseable values): {cleaned_nans}\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a4e94-8702-4a71-9c9f-9d6362bbb11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Cleaning 'highway' column ---\")\n",
    "\n",
    "# Define the hierarchy: Lower index = higher priority\n",
    "highway_hierarchy_list = [\n",
    "    'motorway', 'trunk', 'primary', 'secondary', 'tertiary', 'busway',\n",
    "    'unclassified', 'residential', 'living_street', 'service', 'road',\n",
    "    'cycleway', 'footway', 'pedestrian', 'path', 'steps', 'track', 'emergency_bay'\n",
    "]\n",
    "# Create a mapping from highway type to its priority (index)\n",
    "highway_priority_map = {road_type: i for i, road_type in enumerate(highway_hierarchy_list)}\n",
    "\n",
    "def clean_highway_value(val, priority_map):\n",
    "    if pd.isna(val):\n",
    "        return np.nan # Will be filled later\n",
    "\n",
    "    # Convert to string, lowercase, and strip whitespace\n",
    "    s_val = str(val).lower().strip()\n",
    "\n",
    "    if not s_val: # Handle empty strings after stripping\n",
    "        return np.nan\n",
    "\n",
    "    # Split by comma, then strip whitespace from each part\n",
    "    parts = [p.strip() for p in s_val.split(',')]\n",
    "\n",
    "    processed_parts = []\n",
    "    for part in parts:\n",
    "        if not part: # Skip empty strings resulting from \"a, ,b\"\n",
    "            continue\n",
    "        # Handle _link: e.g., \"secondary_link\" -> \"secondary\"\n",
    "        if part.endswith(\"_link\"):\n",
    "            part = part[:-5] # Remove \"_link\"\n",
    "        processed_parts.append(part)\n",
    "\n",
    "    if not processed_parts: # If all parts were empty or just delimiters\n",
    "        return np.nan\n",
    "\n",
    "    # Find the highest priority part based on the hierarchy\n",
    "    # Assign a very high number (low priority) to types not in the hierarchy map\n",
    "    best_part_found = None\n",
    "    current_highest_priority = float('inf') # Lower value means higher priority\n",
    "\n",
    "    for part in processed_parts:\n",
    "        priority = priority_map.get(part, float('inf')) # Get priority, default to very low if not in map\n",
    "        if priority < current_highest_priority:\n",
    "            current_highest_priority = priority\n",
    "            best_part_found = part\n",
    "        # If two parts have the same priority (e.g. \"primary, primary_link\" -> both become \"primary\"),\n",
    "        # the first one encountered is kept.\n",
    "\n",
    "    # If no part was found in the hierarchy map (e.g. \"random_tag, another_random\"),\n",
    "    # it will return the first part from processed_parts.\n",
    "    # If only one part was present and it's not in hierarchy, it's returned.\n",
    "    if best_part_found is None and processed_parts: # Should only happen if all parts are not in map\n",
    "        return processed_parts[0] # Default to the first processed part if none are in hierarchy\n",
    "        \n",
    "    return best_part_found\n",
    "\n",
    "\n",
    "# Apply the cleaning function\n",
    "df['highway_cleaned'] = df['highway'].apply(lambda x: clean_highway_value(x, highway_priority_map))\n",
    "\n",
    "print(\"\\nValue counts for 'highway_cleaned' (Top 50) before NaN imputation:\")\n",
    "print(df['highway_cleaned'].value_counts(dropna=False).head(50))\n",
    "\n",
    "# K-Prototypes expects categorical features to be strings, and NaNs can cause issues.\n",
    "# Impute NaNs with a specific string category like 'unknown_highway'.\n",
    "df['highway_cleaned'].fillna('unknown_highway', inplace=True)\n",
    "\n",
    "print(\"\\nValue counts for 'highway_cleaned' (Top 50) after NaN imputation:\")\n",
    "print(df['highway_cleaned'].value_counts(dropna=False).head(50))\n",
    "print(f\"\\nData type of 'highway_cleaned': {df['highway_cleaned'].dtype}\")\n",
    "print(f\"Number of unique categories in 'highway_cleaned': {df['highway_cleaned'].nunique()}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb3852-202b-4034-88ca-d017e677459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Cleaning 'maxspeed' column ---\")\n",
    "\n",
    "# Conversion factor\n",
    "MPH_TO_KPH = 1.60934\n",
    "\n",
    "# Textual speed conversion map (KPH values)\n",
    "# These will be converted to string later if they are to be categorical.\n",
    "text_speed_map_kph = {\n",
    "    'ru:urban': 60,\n",
    "    'ro:urban': 50,\n",
    "    'rs:urban': 50,\n",
    "}\n",
    "\n",
    "def parse_single_speed_value(speed_token_str):\n",
    "    \"\"\"\n",
    "    Parses a single speed token (e.g., \"25 mph\", \"RU:urban\", \"50\")\n",
    "    into a numeric KPH value. Returns None if unparseable.\n",
    "    \"\"\"\n",
    "    token = speed_token_str.lower().strip()\n",
    "\n",
    "    if not token:\n",
    "        return None\n",
    "\n",
    "    # 1. Check textual map\n",
    "    if token in text_speed_map_kph:\n",
    "        return float(text_speed_map_kph[token])\n",
    "\n",
    "    # 2. Check for MPH\n",
    "    if 'mph' in token:\n",
    "        # Extract numbers (can handle \"20 mph\" or \"20mph\")\n",
    "        numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", token)\n",
    "        if numbers:\n",
    "            try:\n",
    "                mph_val = float(numbers[0])\n",
    "                kph_val = mph_val * MPH_TO_KPH\n",
    "                # Round to nearest 5 KPH\n",
    "                return round(kph_val / 5.0) * 5.0\n",
    "            except ValueError:\n",
    "                return None # Should not happen if re.findall works\n",
    "        else: # 'mph' present but no number found\n",
    "            return None\n",
    "\n",
    "    # 3. Try direct numeric conversion (assume KPH)\n",
    "    try:\n",
    "        # This handles integers \"50\" and floats \"45.5\"\n",
    "        return float(token)\n",
    "    except ValueError:\n",
    "        return None # Not a direct number, not mph, not in text map\n",
    "\n",
    "\n",
    "def clean_maxspeed_value(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan # Keep NaNs\n",
    "\n",
    "    s_val = str(val).strip()\n",
    "    if not s_val:\n",
    "        return np.nan\n",
    "\n",
    "    # Split by common delimiters like comma or semicolon\n",
    "    # Allows for \"30, 50\" or \"20 mph; 30\"\n",
    "    parts = re.split(r'[,;]', s_val)\n",
    "\n",
    "    kph_values_found = []\n",
    "    for part_str in parts:\n",
    "        if not part_str.strip(): # Skip empty parts from \"val1,,val2\"\n",
    "            continue\n",
    "        numeric_kph = parse_single_speed_value(part_str)\n",
    "        if numeric_kph is not None:\n",
    "            kph_values_found.append(numeric_kph)\n",
    "\n",
    "    if not kph_values_found: # No valid KPH value could be parsed from any part\n",
    "        return np.nan\n",
    "\n",
    "    # Choose the lowest KPH value\n",
    "    final_kph_value = min(kph_values_found)\n",
    "\n",
    "    # Convert to integer if it's a whole number (e.g., 50.0 -> 50), then to string\n",
    "    # This ensures \"50.0\" becomes \"50\" for categorical use.\n",
    "    if final_kph_value == int(final_kph_value):\n",
    "        return str(int(final_kph_value))\n",
    "    else:\n",
    "        # For speeds like 42.5 (if rounding to nearest 2.5 was used, or came from data)\n",
    "        # Keep one decimal place or as is for the string representation.\n",
    "        # For rounding to nearest 5, this case (non-integer) should be rare unless original data had it.\n",
    "        return str(final_kph_value)\n",
    "\n",
    "\n",
    "# Apply the cleaning function\n",
    "df['maxspeed_cleaned'] = df['maxspeed'].apply(clean_maxspeed_value)\n",
    "\n",
    "print(\"\\nValue counts for 'maxspeed_cleaned' (Top 50) before any further NaN handling:\")\n",
    "print(df['maxspeed_cleaned'].value_counts(dropna=False).head(50))\n",
    "\n",
    "print(f\"\\nData type of 'maxspeed_cleaned': {df['maxspeed_cleaned'].dtype}\")\n",
    "print(f\"Number of unique categories in 'maxspeed_cleaned' (excl NaN): {df['maxspeed_cleaned'].nunique()}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326672bc-869f-4703-89ac-09a3109517d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Handling Numerical Features: Imputation and Outlier Treatment ---\")\n",
    "\n",
    "# --- 1. Define Numerical Columns to Process ---\n",
    "# These are the raw numerical columns and the newly cleaned 'lanes_cleaned'\n",
    "# We process source columns for TotalCrossingCount *before* summing.\n",
    "numerical_cols_to_process = [\n",
    "    'lanes_cleaned',                 # Already created and is float (can have NaN)\n",
    "    'traffic_sign_count',            # Original numerical\n",
    "    'highway_traffic_signals_count', # Original numerical\n",
    "    'crossing_traffic_signals_count',# Original numerical (source for TotalCrossingCount)\n",
    "    'crossing_unmarked_count',       # Original numerical (source for TotalCrossingCount)\n",
    "    'crossing_uncontrolled_count',   # Original numerical (source for TotalCrossingCount)\n",
    "    'highway_stop_count',            # Original numerical\n",
    "    'highway_give_way_count'         # Original numerical\n",
    "]\n",
    "\n",
    "# Ensure these columns exist in the DataFrame\n",
    "numerical_cols_to_process = [col for col in numerical_cols_to_process if col in df.columns]\n",
    "if not numerical_cols_to_process:\n",
    "    print(\"No numerical columns found to process. Skipping this section.\")\n",
    "else:\n",
    "    print(f\"Numerical columns to process: {numerical_cols_to_process}\")\n",
    "\n",
    "    # --- 2. Impute NaNs in Numerical Columns ---\n",
    "    print(\"\\n--- Imputing NaNs in Numerical Columns ---\")\n",
    "    for col in numerical_cols_to_process:\n",
    "        if df[col].isnull().any():\n",
    "            original_nan_count = df[col].isnull().sum()\n",
    "            if col == 'lanes_cleaned':\n",
    "                # For 'lanes_cleaned', median imputation is often a good choice.\n",
    "                # Or a specific default like 1 or 2 if that makes more sense.\n",
    "                impute_value = df[col].median()\n",
    "                # Ensure impute_value is not NaN itself if the column is all NaNs\n",
    "                if pd.isna(impute_value):\n",
    "                    impute_value = 1 # Fallback if median is NaN\n",
    "                print(f\"Imputing NaNs in '{col}' with median: {impute_value} (Original NaNs: {original_nan_count})\")\n",
    "            else: # For count columns, 0 is a common and sensible imputation\n",
    "                impute_value = 0\n",
    "                print(f\"Imputing NaNs in '{col}' with 0 (Original NaNs: {original_nan_count})\")\n",
    "            df[col].fillna(impute_value, inplace=True)\n",
    "        else:\n",
    "            print(f\"No NaNs found in '{col}'.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- 3. Visualize Distributions and Identify Outliers ---\n",
    "    print(\"\\n--- Visualizing Numerical Distributions (before outlier treatment) ---\")\n",
    "    for col in numerical_cols_to_process:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Column {col} not found for visualization, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- {col} ---\")\n",
    "        print(df[col].describe())\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[col], kde=True, bins=50)\n",
    "        plt.title(f'Histogram of {col}')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- 4. Apply Outlier Treatment (Capping/Winsorization) ---\n",
    "    print(\"\\n--- Applying Outlier Treatment (Capping) ---\")\n",
    "    # Define capping percentiles.\n",
    "    capping_percentiles = {\n",
    "        'lanes_cleaned': 0.995,\n",
    "        'traffic_sign_count': 0.995,\n",
    "        'highway_traffic_signals_count': 0.995,\n",
    "        'crossing_traffic_signals_count': 0.995,\n",
    "        'crossing_unmarked_count': 0.995,\n",
    "        'crossing_uncontrolled_count': 0.995,\n",
    "        'highway_stop_count': 0.995,\n",
    "        'highway_give_way_count': 0.995\n",
    "    }\n",
    "\n",
    "    for col in numerical_cols_to_process:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Column {col} not found for capping, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Upper Capping\n",
    "        percentile_val = capping_percentiles.get(col, 0.995)\n",
    "        upper_limit = df[col].quantile(percentile_val)\n",
    "\n",
    "        # Ensure upper_limit is not NaN (can happen if column is all NaNs or has very few distinct values)\n",
    "        if pd.isna(upper_limit):\n",
    "            print(f\"Could not determine upper limit for '{col}' (quantile is NaN). Skipping capping for this column.\")\n",
    "            continue\n",
    "            \n",
    "        original_max = df[col].max()\n",
    "        \n",
    "        # Apply capping: values > upper_limit become upper_limit\n",
    "        df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])\n",
    "\n",
    "        df[col] = df[col].round().astype(int) # Round to the nearest whole number and convert back to integer\n",
    "        \n",
    "        capped_count = (df[col] == upper_limit).sum() # Count how many were set to the upper limit\n",
    "        \n",
    "        print(f\"Capped '{col}' at {percentile_val*100:.1f}th percentile ({upper_limit:.2f}). \"\n",
    "              f\"Original max: {original_max:.2f}. Values at new max: {capped_count}.\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- 5. Visualize Distributions (after outlier treatment) ---\n",
    "    print(\"\\n--- Visualizing Numerical Distributions (after outlier treatment) ---\")\n",
    "    for col in numerical_cols_to_process:\n",
    "        if col not in df.columns:\n",
    "            continue # Already printed message if not found\n",
    "            \n",
    "        print(f\"\\n--- {col} (After Capping) ---\")\n",
    "        print(df[col].describe())\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[col], kde=True, bins=50)\n",
    "        plt.title(f'Histogram of {col} (Capped)')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'Boxplot of {col} (Capped)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826fd33-7fc9-4c3e-a1a0-33d1f00bc1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Finalizing Cleaned Columns and Reverting to Original Names ---\")\n",
    "\n",
    "# --- 1. Handle columns that had a '_cleaned' version ---\n",
    "\n",
    "# For 'lanes'\n",
    "if 'lanes_cleaned' in df.columns:\n",
    "    if 'lanes' in df.columns: # If original 'lanes' column still exists\n",
    "        df.drop(columns=['lanes'], inplace=True) # Drop original raw 'lanes'\n",
    "    df.rename(columns={'lanes_cleaned': 'lanes'}, inplace=True) # Rename 'lanes_cleaned' to 'lanes'\n",
    "    print(\"Finalized 'lanes' column from 'lanes_cleaned'.\")\n",
    "else:\n",
    "    print(\"Warning: 'lanes_cleaned' not found. 'lanes' column might not be fully processed as intended.\")\n",
    "\n",
    "# For 'highway'\n",
    "if 'highway_cleaned' in df.columns:\n",
    "    if 'highway' in df.columns:\n",
    "        df.drop(columns=['highway'], inplace=True)\n",
    "    df.rename(columns={'highway_cleaned': 'highway'}, inplace=True)\n",
    "    print(\"Finalized 'highway' column from 'highway_cleaned'.\")\n",
    "else:\n",
    "    print(\"Warning: 'highway_cleaned' not found. 'highway' column might not be fully processed as intended.\")\n",
    "\n",
    "# For 'maxspeed'\n",
    "if 'maxspeed_cleaned' in df.columns:\n",
    "    if 'maxspeed' in df.columns:\n",
    "        df.drop(columns=['maxspeed'], inplace=True)\n",
    "    df.rename(columns={'maxspeed_cleaned': 'maxspeed'}, inplace=True)\n",
    "    print(\"Finalized 'maxspeed' column from 'maxspeed_cleaned'.\")\n",
    "else:\n",
    "    print(\"Warning: 'maxspeed_cleaned' not found. 'maxspeed' column might not be fully processed as intended.\")\n",
    "\n",
    "# --- 2. Verify other directly cleaned columns (if they were not renamed but cleaned in place) ---\n",
    "# These columns are assumed to have been cleaned in place (NaNs, outliers for numericals; NaNs, standardization for categoricals)\n",
    "# No renaming needed for these if cleaned in place. Just a good place for a final check/print.\n",
    "\n",
    "numerical_features_processed_inplace = [\n",
    "    'traffic_sign_count',\n",
    "    'highway_traffic_signals_count',\n",
    "    'crossing_traffic_signals_count',\n",
    "    'crossing_unmarked_count',\n",
    "    'crossing_uncontrolled_count',\n",
    "    'highway_stop_count',\n",
    "    'highway_give_way_count'\n",
    "]\n",
    "print(\"\\nVerifying numerical features processed in place:\")\n",
    "for col in numerical_features_processed_inplace:\n",
    "    if col in df.columns:\n",
    "        print(f\"  '{col}': NaNs = {df[col].isnull().sum()}, dtype = {df[col].dtype}\")\n",
    "    else:\n",
    "        print(f\"  Warning: Expected numerical column '{col}' not found.\")\n",
    "\n",
    "categorical_features_processed_inplace = [\n",
    "    'cycle_lane',\n",
    "    'shared_cycle',\n",
    "    'bus_lane'\n",
    "]\n",
    "print(\"\\nVerifying categorical features processed in place (for HasSpecialLane):\")\n",
    "for col in categorical_features_processed_inplace:\n",
    "    if col in df.columns:\n",
    "        print(f\"  '{col}': NaNs = {df[col].isnull().sum()}, dtype = {df[col].dtype}, \"\n",
    "              f\"Unique values (sample): {df[col].unique()[:5]}\") # Show some unique values\n",
    "    else:\n",
    "        print(f\"  Warning: Expected categorical column '{col}' not found.\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e340ed9-1a4b-4692-9b0b-5200a5c57a10",
   "metadata": {},
   "source": [
    "# Preparing data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f74deb-b13b-4d0e-973c-41ce926581f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create 'HasSpecialLane' (Binary Categorical)\n",
    "# Define what values indicate presence of a special lane\n",
    "cycle_lane_presence = ['yes']\n",
    "shared_cycle_presence = ['yes']\n",
    "bus_lane_presence = ['yes']\n",
    "\n",
    "df['HasSpecialLane'] = (\n",
    "    df['cycle_lane'].isin(cycle_lane_presence) |\n",
    "    df['shared_cycle'].isin(shared_cycle_presence) |\n",
    "    df['bus_lane'].isin(bus_lane_presence)\n",
    ").astype(int) # Convert True/False to 1/0\n",
    "\n",
    "print(\"Created 'HasSpecialLane'. Sample values:\")\n",
    "print(df[['cycle_lane', 'shared_cycle', 'bus_lane', 'HasSpecialLane']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd06a6e-ccea-492e-8967-c9b355729af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create 'TotalCrossingCount' (Numerical)\n",
    "crossing_cols_to_sum = [\n",
    "    'crossing_traffic_signals_count',\n",
    "    'crossing_unmarked_count',\n",
    "    'crossing_uncontrolled_count'\n",
    "]\n",
    "# Ensure columns exist and are numeric. Handle potential NaNs if necessary (sum defaults to skipna=True)\n",
    "df['TotalCrossingCount'] = df[crossing_cols_to_sum].sum(axis=1)\n",
    "\n",
    "print(\"\\nCreated 'TotalCrossingCount'. Sample values:\")\n",
    "print(df[crossing_cols_to_sum + ['TotalCrossingCount']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1e4e7-166f-40ee-a933-c7ba58eec7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the final list of features to keep\n",
    "final_feature_list = [\n",
    "    'traffic_sign_count',             # Numerical\n",
    "    'maxspeed',                       # Categorical\n",
    "    'lanes',                          # Numerical\n",
    "    'highway',                        # Categorical\n",
    "    'highway_traffic_signals_count',  # Numerical\n",
    "    'HasSpecialLane',                 # Categorical (Binary 0/1)\n",
    "    'TotalCrossingCount'              # Numerical\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78c281-74c1-4abf-8baf-3f4da2a94152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create the final DataFrame for clustering\n",
    "df_cluster = df[final_feature_list].copy()\n",
    "\n",
    "print(f\"\\nCreated final DataFrame 'df_cluster ' with {df_cluster.shape[1]} features.\")\n",
    "print(\"Columns:\", df_cluster.columns.tolist())\n",
    "print(\"Shape:\", df_cluster.shape)\n",
    "print(\"First 5 rows of selected data:\")\n",
    "print(df_cluster.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141149a-9efd-410a-976b-778feaec3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Identify Numerical and Categorical Columns ---\n",
    "numerical_cols = ['lanes', 'traffic_sign_count', 'highway_traffic_signals_count', 'TotalCrossingCount']\n",
    "categorical_cols = ['maxspeed', 'highway', 'HasSpecialLane']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_cluster[col] = df_cluster[col].astype(str)\n",
    "\n",
    "# 6. Standardize numerical features (robustly handle missing columns)\n",
    "numerical_cols = [col for col in df_cluster if col not in categorical_cols]\n",
    "# Keep only existing numerical features\n",
    "numerical_cols = [num_col for num_col in numerical_cols if num_col in df_cluster.columns]\n",
    "\n",
    "if numerical_cols:  # Only standardize if there are numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df_cluster[numerical_cols] = scaler.fit_transform(df_cluster[numerical_cols])\n",
    "\n",
    "# Define categorical_indices before using it in fit_predict\n",
    "categorical_indices = [df_cluster.columns.get_loc(col) for col in categorical_cols if col in df_cluster.columns]  # Get indices of categorical columns\n",
    "\n",
    "print(f\"\\nCategorical feature indices for k-Prototypes: {categorical_indices}\")\n",
    "print(f\"Numerical features: {numerical_cols}\")\n",
    "print(f\"Categorical features: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2822e9-aab1-4a04-88b5-2c3458114470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Calculate Variability/Informativeness Scores ---\n",
    "variability_scores = {}\n",
    "\n",
    "# Numerical Variables: Variance\n",
    "print(\"\\n--- Calculating Raw Variance for Numerical Features ---\")\n",
    "variances = df[numerical_cols].var()\n",
    "print(\"Raw Variances:\")\n",
    "print(variances)\n",
    "print(\"-\" * 30)\n",
    "variability_scores.update(variances.to_dict())\n",
    "\n",
    "# Categorical Variables: Entropy\n",
    "print(\"\\n--- Calculating Raw Entropy for Categorical Features ---\")\n",
    "raw_entropies = {}\n",
    "for col in categorical_cols:\n",
    "    if df[col].nunique() <= 1:\n",
    "        # Handle columns with only one category (zero entropy)\n",
    "        score = 0.0\n",
    "    else:\n",
    "        # Calculate probability distribution (frequencies)\n",
    "        p_data = df[col].value_counts(normalize=True)\n",
    "        # Calculate entropy\n",
    "        score = calculate_entropy(p_data)\n",
    "    variability_scores[col] = score\n",
    "    raw_entropies[col] = score # Store raw entropy too for display\n",
    "\n",
    "print(\"Raw Entropies:\")\n",
    "print(pd.Series(raw_entropies))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create a DataFrame for scores\n",
    "scores_df = pd.DataFrame.from_dict(variability_scores, orient='index', columns=['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202b052-c68e-4646-86b4-194036ea75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Normalize Scores ---\n",
    "# Use MinMaxScaler to scale scores to [0, 1]\n",
    "# Reshape scores for the scaler (it expects a 2D array)\n",
    "scores = scores_df['Score'].values.reshape(-1, 1)\n",
    "\n",
    "# Handle potential case where all scores are the same (avoid division by zero)\n",
    "if np.all(scores == scores[0]):\n",
    "     print(\"Warning: All variability/entropy scores are identical. Normalization might not be meaningful.\")\n",
    "     # Assign a uniform normalized score (e.g., 0.5) or handle as needed\n",
    "     normalized_scores = np.full_like(scores, 0.5)\n",
    "else:\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_scores = scaler.fit_transform(scores)\n",
    "\n",
    "scores_df['NormalizedScore'] = normalized_scores\n",
    "scores_df = scores_df.sort_values(by='NormalizedScore', ascending=False)\n",
    "\n",
    "print(\"Variability/Informativeness Scores (Normalized):\")\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f939a-f6e0-4823-a548-a5d7d9afc871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Qualitative Analysis & Selection ---\n",
    "# Note: The following selection methods are for analytical purposes and do not alter df_cluster.\n",
    "# Option A: Threshold\n",
    "threshold = 0.2 # Example threshold\n",
    "selected_features_thresh = scores_df[scores_df['NormalizedScore'] > threshold].index.tolist()\n",
    "print(f\"\\nFeatures that should be selected using threshold > {threshold}:\", selected_features_thresh)\n",
    "\n",
    "# Option B: Top N\n",
    "top_n = 8 # Example: Select top 8 features\n",
    "selected_features_topn = scores_df.head(top_n).index.tolist()\n",
    "print(f\"\\nFeatures that should be selected if picking Top {top_n}:\", selected_features_topn)\n",
    "\n",
    "# Option C: Manual Selection based on plot/domain knowledge\n",
    "scores_df['NormalizedScore'].plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Normalized Variability/Informativeness Scores')\n",
    "plt.ylabel('Normalized Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415417d-1455-4b2d-a251-7904f456dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Check for Redundancy Among Features ---\n",
    "\n",
    "print(\"\\nChecking Redundancy...\")\n",
    "# Numerical Correlation\n",
    "if len(numerical_cols) > 1:\n",
    "    corr_matrix = df[numerical_cols].corr()\n",
    "    print(\"\\nCorrelation Matrix (Selected Numerical):\")\n",
    "    print(corr_matrix)\n",
    "    # Visualize heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Matrix of Selected Numerical Features')\n",
    "    plt.show()\n",
    "    # Decision: If high correlation (e.g., > |0.8|) is found between two variables,\n",
    "    # consider removing one based on domain knowledge or lower variability score.\n",
    "\n",
    "# Categorical Association (using Cramer's V)\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    if min((kcorr-1), (rcorr-1)) == 0:\n",
    "         # Handle cases with zero degrees of freedom (e.g., constant variable)\n",
    "         # This shouldn't happen if we selected based on non-zero entropy/variance,\n",
    "         # but good to have a fallback.\n",
    "         return 0.0\n",
    "    else:\n",
    "         return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "\n",
    "if len(categorical_cols) > 1:\n",
    "    print(\"\\nCramer's V (Selected Categorical):\")\n",
    "    cramer_results = pd.DataFrame(index=categorical_cols, columns=categorical_cols)\n",
    "    for col1 in categorical_cols:\n",
    "        for col2 in categorical_cols:\n",
    "            if col1 == col2:\n",
    "                cramer_results.loc[col1, col2] = 1.0\n",
    "            else:\n",
    "                cramer_results.loc[col1, col2] = cramers_v(df[col1], df[col2])\n",
    "    # Convert to numeric after filling\n",
    "    cramer_results = cramer_results.astype(float)\n",
    "    print(cramer_results)\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cramer_results, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Cramer's V for Selected Categorical Features\")\n",
    "    plt.show()\n",
    "    # Decision: If high Cramer's V (e.g., > 0.7) is found, consider removing one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d623d-7411-4ba7-9e24-9de9ae665bff",
   "metadata": {},
   "source": [
    "# DETERMINING OPTIMAL K, USING 2 METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36516c-5d75-43b2-82d2-5cbfeb5f683a",
   "metadata": {},
   "source": [
    "## Determining optimal k using the Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e189d7-31ab-4435-8d98-96c7b9ad6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "K = range(1, 11)  # Test cluster numbers from 1 to 10 (adjust range as needed)\n",
    "\n",
    "for num_clusters in list(K): #iterate through cluster numbers\n",
    "    print(f\"Running Cluster {num_clusters}:\")\n",
    "    kproto = KPrototypes(n_clusters=num_clusters, init='Cao', verbose=2, random_state=0, n_jobs=-1) #n_jobs=-1 uses all available cores\n",
    "    kproto.fit_predict(df_cluster, categorical=categorical_indices)\n",
    "    costs.append(kproto.cost_) #Append cost to costs list\n",
    "\n",
    "plt.plot(K, costs, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Cost')\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'elbow_method.png'))\n",
    "plt.show() #Plot elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d204c64-3af2-4db9-bfcc-75e42141e61a",
   "metadata": {},
   "source": [
    "## Determining Optimal k using the Silhouette Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8024b-087b-46b5-b7e7-a99baf9f0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting the Silhouette Method\")\n",
    "silhouette_avg = []\n",
    "K = range(2, 11)  # Adjust range as needed\n",
    "\n",
    "for num_clusters in list(K):\n",
    "    print(f\"Running Cluster {num_clusters}:\")\n",
    "    kproto = KPrototypes(n_clusters=num_clusters, init='Cao', verbose=2, random_state=0, n_jobs=-1)\n",
    "    cluster_labels = kproto.fit_predict(df_cluster, categorical=categorical_indices)\n",
    "\n",
    "    # --- One-hot encode categorical features before calculating Silhouette score ---\n",
    "    df_silhouette = df_cluster.copy()  # Create a copy for one-hot encoding\n",
    "    df_silhouette = pd.get_dummies(df_silhouette, columns=categorical_cols)\n",
    "    silhouette_avg = silhouette_score(df_silhouette.values, cluster_labels)  # Average score for all samples\n",
    "\n",
    "    sample_silhouette_values = silhouette_samples(df_silhouette.values, cluster_labels) #Silhouette values for each sample\n",
    "\n",
    "    # --- Create the Silhouette plot ---\n",
    "    n_clusters = num_clusters  # Assign num_clusters to n_clusters so it is defined in this scope.\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(7, 5)\n",
    "    ax1.set_xlim([-0.1, 1])  #xlim and ylim values are fixed, adapt if needed\n",
    "    ax1.set_ylim([0, len(df_silhouette) + (n_clusters + 1) * 10]) # adapt if needed\n",
    "    y_lower = 10\n",
    "\n",
    "    for i in range(n_clusters): #Iterate through cluster labels\n",
    "\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] #Silhouette scores for current cluster\n",
    "        ith_cluster_silhouette_values.sort() #Sort silhouette scores\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0] #number of samples in cluster i\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "\n",
    "        color = plt.cm.nipy_spectral(float(i) / n_clusters)  # Customize colors if needed\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) #Label the silhouette plots with their cluster numbers at the middle\n",
    "\n",
    "\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KPrototypes clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'silhouette_full_data_k_{n_clusters}.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9983c7d-8e3a-4251-9260-b17d39dd3e4e",
   "metadata": {},
   "source": [
    "## Determining Optimal k using the Silhouette Method on a Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4921ba9-2ed6-4ff5-8d31-e1eecdeefd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting the Silhouette Method using Sampling\")\n",
    "silhouette_scores_avg = [] # Store average scores for each k\n",
    "K = range(3, 6)  # Adjust range as needed\n",
    "SAMPLE_SIZE = 700000  # Or 5000, 20000 etc. Adjust based on your RAM. Needs to be large enough to be representative.\n",
    "if SAMPLE_SIZE > len(df_cluster):\n",
    "    SAMPLE_SIZE = len(df_cluster) # Don't sample more than available data\n",
    "\n",
    "for num_clusters in K:\n",
    "    print(f\"\\nRunning KPrototypes for k = {num_clusters} on full data...\")\n",
    "    kproto = KPrototypes(n_clusters=num_clusters, init='Cao', verbose=2, random_state=0, n_jobs=-1) # Use all cores\n",
    "\n",
    "    # Fit on FULL data, predict on FULL data\n",
    "    cluster_labels = kproto.fit_predict(df_cluster.values, categorical=categorical_indices)\n",
    "    print(f\"KPrototypes done for k = {num_clusters}.\")\n",
    "\n",
    "    # --- Perform Sampling for Silhouette ---\n",
    "    print(f\"Sampling {SAMPLE_SIZE} points for Silhouette calculation...\")\n",
    "    if len(df_cluster) > SAMPLE_SIZE:\n",
    "        # Create random indices\n",
    "        sample_indices = np.random.choice(df_cluster.index, SAMPLE_SIZE, replace=False)\n",
    "        # Select sample data AND corresponding labels\n",
    "        df_sample = df_cluster.loc[sample_indices]\n",
    "        labels_sample = cluster_labels[sample_indices]\n",
    "    else:\n",
    "        # If dataset is smaller than sample size, use all data\n",
    "        df_sample = df_cluster\n",
    "        labels_sample = cluster_labels\n",
    "\n",
    "    print(\"One-hot encoding the sample...\")\n",
    "    # One-hot encode ONLY the sample\n",
    "    df_silhouette_sample = pd.get_dummies(df_sample, columns=categorical_cols)\n",
    "\n",
    "    print(\"Calculating Silhouette score on the sample...\")\n",
    "    # Calculate Silhouette score using ONLY the sample data and labels\n",
    "    silhouette_avg = silhouette_score(df_silhouette_sample.values, labels_sample)  # Average score for all samples\n",
    "    silhouette_scores_avg.append(silhouette_avg)\n",
    "    print(f\"Average Silhouette Score for k = {num_clusters}: {silhouette_avg:.4f}\")\n",
    "    sample_silhouette_values = silhouette_samples(df_silhouette_sample.values, labels_sample) #Silhouette values for each sample\n",
    "      \n",
    "    # --- Create the Silhouette plot ---\n",
    "    print(\"Start plotting...\")\n",
    "    n_clusters = num_clusters  # Assign num_clusters to n_clusters so it is defined in this scope.\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(7, 5)\n",
    "    ax1.set_xlim([-0.1, 1])  #xlim and ylim values are fixed, adapt if needed\n",
    "    ax1.set_ylim([0, len(df_silhouette_sample) + (n_clusters + 1) * 10]) # adapt if needed\n",
    "    y_lower = 10\n",
    "\n",
    "    for i in range(n_clusters): #Iterate through cluster labels\n",
    "\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[labels_sample == i] #Silhouette scores for current cluster\n",
    "        ith_cluster_silhouette_values.sort() #Sort silhouette scores\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0] #number of samples in cluster i\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "\n",
    "        color = plt.cm.nipy_spectral(float(i) / n_clusters)  # Customize colors if needed\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) #Label the silhouette plots with their cluster numbers at the middle\n",
    "\n",
    "\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KPrototypes clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'silhouette_sampled_k_{n_clusters}.png'))\n",
    "    plt.close(fig) # Close the figure to free up memory\n",
    "\n",
    "    # --- Optional: Clean up large objects ---\n",
    "    del df_sample, labels_sample, df_silhouette_sample, kproto, cluster_labels\n",
    "    gc.collect() # Force garbage collection (might help, might not)\n",
    "\n",
    "# --- Plotting the results (handling potential NaNs) ---\n",
    "valid_indices = ~np.isnan(silhouette_scores_avg) # Find where scores are not NaN\n",
    "valid_K = np.array(list(K))[valid_indices]\n",
    "valid_scores = np.array(silhouette_scores_avg)[valid_indices]\n",
    "\n",
    "# --- Plotting the results (Average Silhouette Scores vs K) ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K, silhouette_scores_avg, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Average Silhouette Score (Sampled)')\n",
    "plt.title('Silhouette Score for Determining Optimal k (using Sampling)')\n",
    "plt.grid(True)\n",
    "plt.xticks(list(K))\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'silhouette_method_average_scores_sampled.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f667f1-23ff-4bfb-a03e-345bb9d38769",
   "metadata": {},
   "source": [
    "# K-Prototypes clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73c506-9bee-433c-aea7-168e972e9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose optimal k based on the elbow plot ---\n",
    "# Examine the Elbow and Silhouette performed before and select the k value at the \"elbow\" point\n",
    "# (where the rate of decrease in cost starts to slow down significantly).\n",
    "#Then, update below the value of n_clusters\n",
    "\n",
    "n_clusters=4\n",
    "\n",
    "# --- Perform K-Prototypes clustering with the optimal k ---\n",
    "print(f\"Running K-Prototypes with {n_clusters} clusters.\")\n",
    "kproto = KPrototypes(n_clusters=n_clusters, init='Cao', verbose=2, random_state=0, n_jobs=-1)\n",
    "clusters = kproto.fit_predict(df_cluster, categorical=categorical_indices)\n",
    "\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# --- Analyze cluster characteristics (with debugging prints) ---\n",
    "print(df['cluster'].value_counts())  # Number of edges in each cluster\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    cluster_data = df[df['cluster'] == i]\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    #print(f\"cluster_data.dtypes:\\n{cluster_data.dtypes}\") #Print data types\n",
    "    #print(f\"cluster_data head:\\n{cluster_data.head()}\") # Print head of cluster_data\n",
    "\n",
    "    if cluster_data.empty: #check if empty and skip if so\n",
    "        print(f\"cluster_data is empty. Skipping cluster {i}\")\n",
    "        continue\n",
    "\n",
    "    for feature in final_feature_list:\n",
    "        \n",
    "        if cluster_data[feature].apply(type).eq(list).any():\n",
    "          print(f\"  {feature}:\")\n",
    "          exploded_counts = cluster_data.explode(feature)[feature].value_counts().to_dict()\n",
    "          for val, count in exploded_counts.items():\n",
    "            print(f\"    {val}: {count}\")    \n",
    "        else:  # Proceed as usual if no lists found\n",
    "            print(f\"  {feature}: {cluster_data[feature].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"Clustering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c4a9f-f8d4-4180-be00-0fa184bd8bb1",
   "metadata": {},
   "source": [
    "# Saving final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47503e2c-994e-4f13-8640-8fd3dd559038",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results_gpkg = os.path.join(RESULTS_DIR, \"full_network_with_clusters.gpkg\")\n",
    "cluster_results_csv = os.path.join(RESULTS_DIR, \"full_network_with_clusters.csv\")\n",
    "\n",
    "# Drop city_id before final save\n",
    "df_to_save = df.drop(columns=['city_id'], errors='ignore')\n",
    "\n",
    "# --- 1. Save to GeoPackage (GPKG) ---\n",
    "try:\n",
    "    print(f\"Saving clustered network with geometry to: {cluster_results_gpkg}\")\n",
    "    df_to_save.to_file(cluster_results_gpkg, driver=\"GPKG\")\n",
    "except Exception as e_save_gpkg:\n",
    "    print(f\"Error saving clustered GPKG: {e_save_gpkg}\")\n",
    "\n",
    "# --- 2. Save to Comma-Separated Values (CSV) ---\n",
    "try:\n",
    "    print(f\"Saving clustered attributes to: {cluster_results_csv}\")\n",
    "    df_to_save.to_csv(cluster_results_csv, sep=\";\", index=False)\n",
    "except Exception as e_csv:\n",
    "    print(f\"Error saving clustered CSV: {e_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7e81d-29b5-4ecd-a530-45f6f075d261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
